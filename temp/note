1.损失函数的值约20000,发现是crf里面_score_sentence计算的gold_score非常大，原因是我在data里面人为给序列加上了起始和结束符号，在NCRFpp里面起始是没有的。去掉人为的起始结束符号就行了。
2.在bert后面的全连接层之前加上dropout效果好像会稳定些
3.bert参数的学习率设置为0.00001，weight_decay为0
4.用nn.DataParallel会导致“StopIteration: Caught StopIteration in replica 0 on device 0.”错误，参考这里的描述 https://github.com/pytorch/pytorch/issues/40457
5.torch中的Quantization技术：https://pytorch.org/docs/stable/quantization.html?highlight=quantized
    https://pytorch.org/docs/stable/torch.nn.quantized.html?highlight=quantized#module-torch.nn.quantized
6.tensor的requires_grad和is_leaf属性
7.自动求导 https://pytorch.org/docs/stable/autograd.html#torch.Tensor.requires_grad
8.torch里的dropout在执行后会自动对向量进行1/(1-p)的放大！！具体参见torch的文档！！
9.jieba分词cut_all=True的bug似乎在python3.6中没有，在python3.7中有。
10.各种模型在msra数据集上都很不稳定，在测试集上的准确率上窜下跳，虽然之前在people_daily_1998上都很稳定，可能是因为people_daily_1998有时间和数字这两个容易识别的实体占大头的缘故


自己的lstm和torch的lstm的对比：
my_lstm       : ml   (自己的lstm，否则torch的lstm)
my_cell       : mc   (自己的lstmcell，否则torch的lstmcell)
dropout_h_out : dho  (对输出的h作dropout，否则不作dropout)
dropout_h_next: dhn  (对传到下一个单元的h作dropout，否则不作dropout)
dropout_c_next: dcn  (对传到下一个单元的c作dropout，否则不作dropout)
bidirectional : bid  (双向lstm，否则单向lstm)
epoch = 48
my_lstm, my_cell, dropout_h_out, dropout_h_next, dropout_c_next, bidirectional, dropout,         f1
      1,       1,             1,              0,              0,             0,     0.5,   0.810074
      1,       1,             1,              1,              0,             0,     0.5,   0.768115
      1,       1,             1,              0,              0,             1,     0.5,   0.856157
      1,       1,             1,              1,              0,             1,     0.5,   0.823424
      1,       0,             1,              0,              0,             0,     0.5,   0.805177
      1,       0,             1,              0,              0,             1,     0.5,   0.863181
      0,       0,             x,              x,              x,             0,     0.5,   0.863249
      0,       0,             x,              x,              x,             1,     0.5,   0.894212
      1,       0,             1,              0,              0,             0,       0,   0.869831
      1,       0,             1,              0,              0,             1,       0,   0.897052
      0,       0,             x,              x,              x,             0,       0,   0.869188
      0,       0,             x,              x,              x,             1,       0,   0.893086
      1,       1,             1,              0,              0,             0,       0,   0.865011
      1,       1,             1,              0,              0,             1,       0,   0.898622
